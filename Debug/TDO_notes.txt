
# Potential optimizations:

    - in the first iteration, since all elemental stiffness matrices are the same, use this one single one to assemble the global grid
        - while assembly and the first iteration is done, concurrently copy the remaining element stiffness matrices, so that by the time the first iteration is done, each of these local stiffness matrices can be updated with its density variable

    - assembleGrid_GPU in assemble.cu
        - now you are doing a for loop for each thread id, meaning, one thread handles one row, which scans through the columns
        - you wanted to try and have each thread handles ONE matrix cell, you're stuck here
            - this is a potential optimization
        - some refresher :

            setAt( 2*node_index[i], 2*node_index[j], g_value, g_index, g_max_row_size, valueAt( 2*i, 2*j, l_value, l_index, l_max_row_size) );
                    |
                    -> here, i is the row of the global matrix.
                        - in this for loop, j is increased because we're scanning along the columns. i remains static
                
            NOTE: the for loop runs two functions:

                    setAt( 2*node_index[i], 2*node_index[j], g_value, g_index, g_max_row_size, valueAt( 2*i, 2*j, l_value, l_index, l_max_row_size) );
                    setAt( 2*node_index[i], 2*node_index[j] + 1 , g_value, g_index, g_max_row_size, valueAt( 2*i, 2*j + 1 , l_value, l_index, l_max_row_size) );
        - NOTE: look into running the kernels in a 2D grid

    - while assembling in CPU, have GPU malloc and memcpy stuff, like residuum and displacement vectors, gmg multilevel matrices,

    - get average iterations of bisection, and don't do the foo memcpy, but do a fixed iteration times

    - while(foo) boolean, don't do the memcpy, but get an estimate of how many iterations needed
        - draw a graph ( # of iterations vs N size ), and find the linear correlation
        - have a kernel in the end to check the final result, if "failed" is returned, display a message to increase # of iterations

    - this foo loop
        - check first in device if it's convergent,
            - if not convergent, don't send the d_foo to host, but instead continue with loop
            - if it's convergent, send the d_foo to host, and the host will stop the loop




https://www.youtube.com/watch?v=I-W9jw7fwOg&list=PLqMASTX2e0SZb_ca77KBAHy10pz5vWEMp&index=5&t=0s

TODO: read these:
    https://devblogs.nvidia.com/how-overlap-data-transfers-cuda-cc/
    https://devblogs.nvidia.com/faster-parallel-reductions-kepler/
    https://devblogs.nvidia.com/finite-difference-methods-cuda-cc-part-1/   (from prof)
    https://devblogs.nvidia.com/efficient-matrix-transpose-cuda-cc/


# archive

double B[8][6][24] = {  {   {-1.3958,	0,	0,	1.2033,	0,	0,	-0.374,	0,	0,	0.32244,	0,	0,	-0.18155,	0,	0,	0.374,	0,	0,	-0.048647,	0,	0,	0.10021,	0,	0}, 
                                {0,	-1.4143,	0,	0,	-0.18651,	0,	0,	1.1984,	0,	0,	0.37267,	0,	0,	-0.35548,	0,	0,	-0.043685,	0,	0,	0.3274,	0,	0,	0.10154,	0},
                                {0,	0,	-1.244,	0,	0,	-0.33333,	0,	0,	-0.33333,	0,	0,	-0.089316,	0,	0,	1.244,	0,	0,	0.33333,	0,	0,	0.33333,	0,	0,	0.089316}.
                                {-1.4143,	-1.3958,	0,	-0.18651,	1.2033,	0,	1.1984,	-0.374,	0,	0.37267,	0.32244,	0,	-0.35548,	-0.18155,	0,	-0.043685,	0.374,	0,	0.3274,	-0.048647,	0,	0.10154,	0.10021,	0},
                                {0,	-1.244,	-1.4143,	0,	-0.33333,	-0.18651,	0,	-0.33333,	1.1984,	0,	-0.089316,	0.37267,	0,	1.244,	-0.35548,	0,	0.33333,	-0.043685,	0,	0.33333,	0.3274,	0,	0.089316,	0.10154},
                                {-1.244,	0,	-1.3958,	-0.33333,	0,	1.2033,	-0.33333,	0,	-0.374,	-0.089316,	0,	0.32244,	1.244,	0,	-0.18155,	0.33333,	0,	0.374,	0.33333,	0,	-0.048647,	0.089316,	0,	0.10021} },
                            {   {-1.2847,	0,	0,	1.0922,	0,	0,	-0.34423,	0,	0,	0.29266,	0,	0,	-0.29266,	0,	0,	0.48511,	0,	0,	-0.078419,	0,	0,	0.12999,	0,	0},
                                {0,	-0.17659,	0,	0,	-1.3773,	0,	0,	0.37533,	0,	0,	1.2083,	0,	0,	-0.053609,	0,	0,	-0.39252,	0,	0,	0.098884,	0,	0,	0.31747,	0},
                                {0,	0,	-0.33333,	0,	0,	-1.244,	0,	0,	-0.089316,	0,	0,	-0.33333,	0,	0,	0.33333,	0,	0,	1.244,	0,	0,	0.089316,	0,	0,	0.33333},
                                {-0.17659,	-1.2847,	0,	-1.3773,	1.0922,	0,	0.37533,	-0.34423,	0,	1.2083,	0.29266,	0,	-0.053609,	-0.29266,	0,	-0.39252,	0.48511,	0,	0.098884,	-0.078419,	0,	0.31747,	0.12999,	0},
                                {0,	-0.33333,	-0.17659,	0,	-1.244,	-1.3773,	0,	-0.089316,	0.37533,	0,	-0.33333,	1.2083,	0,	0.33333,	-0.053609,	0,	1.244,	-0.39252,	0,	0.089316,	0.098884,	0,	0.33333,	0.31747},
                                {-0.33333,	0,	-1.2847,	-1.244,	0,	1.0922,	-0.089316,	0,	-0.34423,	-0.33333,	0,	0.29266,	0.33333,	0,	-0.29266,	1.244,	0,	0.48511,	0.089316,	0,	-0.078419,	0.33333,	0,	0.12999}},
                            {   {-0.29266,	0,	0,	0.34423,	0,	0,	-1.0922,	0,	0,	1.2847,	0,	0,	-0.12999,	0,	0,	0.078419,	0,	0,	-0.48511,	0,	0,	0.29266,	0,	0},
                                {0,	-1.2797,	0,	0,	-0.29133,	0,	0,	1.1108,	0,	0,	0.49008,	0,	0,	-0.34919,	0,	0,	-0.079749,	0,	0,	0.27415,	0,	0,	0.12502,	0},
                                {0,	0,	-0.33333,	0,	0,	-0.089316,	0,	0,	-1.244,	0,	0,	-0.33333,	0,	0,	0.33333,	0,	0,	0.089316,	0,	0,	1.244,	0,	0,	0.33333},
                                {-1.2797,	-0.29266,	0,	-0.29133,	0.34423,	0,	1.1108,	-1.0922,	0,	0.49008,	1.2847,	0,	-0.34919,	-0.12999,	0,	-0.079749,	0.078419,	0,	0.27415,	-0.48511,	0,	0.12502,	0.29266,	0},
                                {0,	-0.33333,	-1.2797,	0,	-0.089316,	-0.29133,	0,	-1.244,	1.1108,	0,	-0.33333,	0.49008,	0,	0.33333,	-0.34919,	0,	0.089316,	-0.079749,	0,	1.244,	0.27415,	0,	0.33333,	0.12502},
                                {-0.33333,	0,	-0.29266,	-0.089316,	0,	0.34423,	-1.244,	0,	-1.0922,	-0.33333,	0,	1.2847,	0.33333,	0,	-0.12999,	0.089316,	0,	0.078419,	1.244,	0,	-0.48511,	0.33333,	0,	0.29266}},
                            {   {-0.32244,	0,	0,	0.374,	0,	0,	-1.2033,	0,	0,	1.3958,	0,	0,	-0.10021,	0,	0,	0.048647,	0,	0,	-0.374,	0,	0,	0.18155,	0,	0},
                                {0,	-0.29399,	0,	0,	-1.2896,	0,	0,	0.48015,	0,	0,	1.0737,	0,	0,	-0.077089,	0,	0,	-0.33927,	0,	0,	0.13495,	0,	0,	0.31118,	0},
                                {0,	0,	-0.089316,	0,	0,	-0.33333,	0,	0,	-0.33333,	0,	0,	-1.244,	0,	0,	0.089316,	0,	0,	0.33333,	0,	0,	0.33333,	0,	0,	1.244},
                                {-0.29399,	-0.32244,	0,	-1.2896,	0.374,	0,	0.48015,	-1.2033,	0,	1.0737,	1.3958,	0,	-0.077089,	-0.10021,	0,	-0.33927,	0.048647,	0,	0.13495,	-0.374,	0,	0.31118,	0.18155,	0},
                                {0,	-0.089316,	-0.29399,	0,	-0.33333,	-1.2896,	0,	-0.33333,	0.48015,	0,	-1.244,	1.0737,	0,	0.089316,	-0.077089,	0,	0.33333,	-0.33927,	0,	0.33333,	0.13495,	0,	1.244,	0.31118},
                                {-0.089316,	0,	-0.32244,	-0.33333,	0,	0.374,	-0.33333,	0,	-1.2033,	-1.244,	0,	1.3958,	0.089316,	0,	-0.10021,	0.33333,	0,	0.048647,	0.33333,	0,	-0.374,	1.244,	0,	0.18155}},
                            {   {-0.89979,	0,	0,	0.18155,	0,	0,	-0.2411,	0,	0,	0.048647,	0,	0,	-0.67756,	0,	0,	1.3958,	0,	0,	-0.18155,	0,	0,	0.374,	0,	0},
                                {0,	-0.74304,	0,	0,	-0.0066478,	0,	0,	0.22355,	0,	0,	0.11147,	0,	0,	-1.5525,	0,	0,	0.30223,	0,	0,	1.1613,	0,	0,	0.50363,	0},
                                {0,	0,	-1.244,	0,	0,	-0.33333,	0,	0,	-0.33333,	0,	0,	-0.089316,	0,	0,	1.244,	0,	0,	0.33333,	0,	0,	0.33333,	0,	0,	0.089316},
                                {-0.74304,	-0.89979,	0,	-0.0066478,	0.18155,	0,	0.22355,	-0.2411,	0,	0.11147,	0.048647,	0,	-1.5525,	-0.67756,	0,	0.30223,	1.3958,	0,	1.1613,	-0.18155,	0,	0.50363,	0.374,	0},
                                {0,	-1.244,	-0.74304,	0,	-0.33333,	-0.0066478,	0,	-0.33333,	0.22355,	0,	-0.089316,	0.11147,	0,	1.244,	-1.5525,	0,	0.33333,	0.30223,	0,	0.33333,	1.1613,	0,	0.089316,	0.50363},
                                {-1.244,	0,	-0.89979,	-0.33333,	0,	0.18155,	-0.33333,	0,	-0.2411,	-0.089316,	0,	0.048647,	1.244,	0,	-0.67756,	0.33333,	0,	1.3958,	0.33333,	0,	-0.18155,	0.089316,	0,	0.374}},
                            {   {-0.48511,	0,	0,	-0.23312,	0,	0,	-0.12999,	0,	0,	-0.062464,	0,	0,	-1.0922,	0,	0,	1.3958,	0,	0,	-0.29266,	0,	0,	0.48511,	0,	0},
                                {0,	0.13158,	0,	0,	-0.22718,	0,	0,	0.1485,	0,	0,	0.36178,	0,	0,	0.16401,	0,	0,	0.30223,	0,	0,	0.4666,	0,	0,	1.0231,	0},
                                {0,	0,	-0.33333,	0,	0,	-1.244,	0,	0,	-0.089316,	0,	0,	-0.33333,	0,	0,	0.33333,	0,	0,	0.33333,	0,	0,	0.089316,	0,	0,	0.33333},
                                {0.13158,	-0.48511,	0,	-0.22718,	-0.23312,	0,	0.1485,	-0.12999,	0,	0.36178,	-0.062464,	0,	0.16401,	-1.0922,	0,	0.30223,	1.3958,	0,	0.4666,	-0.29266,	0,	1.0231,	0.48511,	0},
                                {0,	-0.33333,	0.13158,	0,	-1.244,	-0.22718,	0,	-0.089316,	0.1485,	0,	-0.33333,	0.36178,	0,	0.33333,	0.16401,	0,	0.33333,	0.30223,	0,	0.089316,	0.4666,	0,	0.33333,	1.0231},
                                {-0.33333,	0,	-0.48511,	-1.244,	0,	-0.23312,	-0.089316,	0,	-0.12999,	-0.33333,	0,	-0.062464,	0.33333,	0,	-1.0922,	0.33333,	0,	1.3958,	0.089316,	0,	-0.29266,	0.33333,	0,	0.48511}},
                            {   {0.062464,	0,	0,	0.12999,	0,	0,	0.23312,	0,	0,	0.48511,	0,	0,	-0.48511,	0,	0,	0.29266,	0,	0,	-1.8105,	0,	0,	1.0922,	0,	0},
                                {0,	-0.30489,	0,	0,	-0.030128,	0,	0,	0.43948,	0,	0,	0.31021,	0,	0,	-1.4649,	0,	0,	-0.20007,	0,	0,	0.41963,	0,	0,	0.83067,	0},
                                {0,	0,	-0.33333,	0,	0,	-0.089316,	0,	0,	-1.244,	0,	0,	-0.33333,	0,	0,	0.33333,	0,	0,	0.089316,	0,	0,	1.244,	0,	0,	0.33333},
                                {-0.30489,	0.062464,	0,	-0.030128,	0.12999,	0,	0.43948,	0.23312,	0,	0.31021,	0.48511,	0,	-1.4649,	-0.48511,	0,	-0.20007,	0.29266,	0,	0.41963,	-1.8105,	0,	0.83067,	1.0922,	0},
                                {0,	-0.33333,	-0.30489,	0,	-0.089316,	-0.030128,	0,	-1.244,	0.43948,	0,	-0.33333,	0.31021,	0,	0.33333,	-1.4649,	0,	0.089316,	-0.20007,	0,	1.244,	0.41963,	0,	0.33333,	0.83067},
                                {-0.33333,	0,	0.062464,	-0.089316,	0,	0.12999,	-1.244,	0,	0.23312,	-0.33333,	0,	0.48511,	0.33333,	0,	-0.48511,	0.089316,	0,	0.29266,	1.244,	0,	-1.8105,	0.33333,	0,	1.0922}},
                            {   {-0.048647,	0,	0,	0.2411,	0,	0,	-0.18155,	0,	0,	0.89979,	0,	0,	-0.374,	0,	0,	0.18155,	0,	0,	-1.3958,	0,	0,	0.67756,	0,	0},
                                {0,	-0.067165,	0,	0,	-0.44311,	0,	0,	0.17198,	0,	0,	-0.076377,	0,	0,	-0.16303,	0,	0,	-1.3267,	0,	0,	0.9689,	0,	0,	0.93549,	0},
                                {0,	0,	-0.089316,	0,	0,	-0.33333,	0,	0,	-0.33333,	0,	0,	-1.244,	0,	0,	0.089316,	0,	0,	0.33333,	0,	0,	0.33333,	0,	0,	1.244},
                                {-0.067165,	-0.048647,	0,	-0.44311,	0.2411,	0,	0.17198,	-0.18155,	0,	-0.076377,	0.89979,	0,	-0.16303,	-0.374,	0,	-1.3267,	0.18155,	0,	0.9689,	-1.3958,	0,	0.93549,	0.67756,	0},
                                {0,	-0.089316,	-0.067165,	0,	-0.33333,	-0.44311,	0,	-0.33333,	0.17198,	0,	-1.244,	-0.076377,	0,	0.089316,	-0.16303,	0,	0.33333,	-1.3267,	0,	0.33333,	0.9689,	0,	1.244,	0.93549},
                                {-0.089316,	0,	-0.048647,	-0.33333,	0,	0.2411,	-0.33333,	0,	-0.18155,	-1.244,	0,	0.89979,	0.089316,	0,	-0.374,	0.33333,	0,	0.18155,	0.33333,	0,	-1.3958,	1.244,	0,	0.67756}}};


# Issues:



TODO:       norm() add tree-like when adding the first threads of each block

TODO:       idea of optimization:
                - while foo is copied back and forth to host, do an async memcpy and continue with calculation
                    - doesn't matter if it's wasteful, can just quit if it has converged

            

CHECK:      When calculating the max_row_size on the device, we have to copy result back to CPU so that CPU will cudamalloc and cpy the required size for the vectors value and index
                - maybe not worthwhile to have it on the GPU?


TODO:       boundary condition (identity row)



TODO:       repair some reduction functions, add this in the beginning:

            __global__ 
            void sumOfVector_GPU(double* sum, double* x, size_t n)
            {
                int id = blockDim.x * blockIdx.x + threadIdx.x;
                int stride = blockDim.x*gridDim.x;
                
                __shared__ double cache[1024];
                cache[threadIdx.x] = 0;    < --- NOTE:
                
                double temp = 0.0;
                while(id < n)
                {
                    temp += x[id];


DONE:       repair PSFEM's calculateDimensions

            __host__ 
            void calculateDimensions(size_t N, dim3 &blockDim, dim3 &gridDim)   // TODO: have it gridDim first --> N, gridDim, blockDim
            {
                if ( N <= 1024 )
                {
                    blockDim.x = 1024; blockDim.y = 1; blockDim.z = 1; //NOTE: here, change N -> 1024, so that reductions work
                    gridDim.x  = 1; gridDim.y = 1; gridDim.z = 1;
                }

            .. because reduction kernels need full blocks to work



DONE:       use full u to calculate local driving forces
                - have a kernel to get only the local displacements








Questions:
- bisection algo

vtk function
    - value should be the design variable, chi?

- convergence on the second iteration should be about the same as the first, right?
    - first: 15 steps
    - second: 150+ steps

    - run on matlab with updated stiffness, get same 150 steps

presentation thurs 12.15h
    - slurm, benchmark
    - Herr Prager - key, next to secretary

hiwi:
latex : best practices masters thesis 
    - reference
    - latex sources
- document:
    - where to find latex templates
    - links to latex resources
    - information on statement / declaration
        - check prof's email
    - info on how the structure should be (supervisors)
    - typical chapter overview
        - overviews, lit review, methods, ...
        - what to be covered in each chapter
    - a few general remarks on style
        - figures need to be captioned
        - format to references
    - typical size of a thesis
    - submitting to compeeng
    - printing a thesis



hiwi:
- thesis guideline
- hpcg benchmark
    - lower flops, will try again


thesis:
- thesis flow
- no update on progress
- see anything wrong with my result?
- any recommendations on tackling this issue of non-converging?
    - handling big matrices

recommendation letter
 - prof write or i draft it?
 - want to mention coding
 - rate code level


PTAP slow.
- since will be doing matrix-less later, should I ...
    - repair this by using ELLpack for assembly
        - if so, max_row_size question
    - start with matrix-less
        - copy only local stiffness matrix to device?
        - P and R matrices?
