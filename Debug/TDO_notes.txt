
# Potential optimizations:

    - in the first iteration, since all elemental stiffness matrices are the same, use this one single one to assemble the global grid
        - while assembly and the first iteration is done, concurrently copy the remaining element stiffness matrices, so that by the time the first iteration is done, each of these local stiffness matrices can be updated with its density variable

    - assembleGrid_GPU in assemble.cu
        - now you are doing a for loop for each thread id, meaning, one thread handles one row, which scans through the columns
        - you wanted to try and have each thread handles ONE matrix cell, you're stuck here
            - this is a potential optimization
        - some refresher :

            setAt( 2*node_index[i], 2*node_index[j], g_value, g_index, g_max_row_size, valueAt( 2*i, 2*j, l_value, l_index, l_max_row_size) );
                    |
                    -> here, i is the row of the global matrix.
                        - in this for loop, j is increased because we're scanning along the columns. i remains static
                
            NOTE: the for loop runs two functions:

                    setAt( 2*node_index[i], 2*node_index[j], g_value, g_index, g_max_row_size, valueAt( 2*i, 2*j, l_value, l_index, l_max_row_size) );
                    setAt( 2*node_index[i], 2*node_index[j] + 1 , g_value, g_index, g_max_row_size, valueAt( 2*i, 2*j + 1 , l_value, l_index, l_max_row_size) );
        - NOTE: look into running the kernels in a 2D grid


    - while assembling in CPU, have GPU malloc and memcpy stuff, like residuum and displacement vectors, gmg multilevel matrices,

    - get average iterations of bisection, and don't do the foo memcpy, but do a fixed iteration times

    - while(foo) boolean, don't do the memcpy, but get an estimate of how many iterations needed
        - draw a graph ( # of iterations vs N size ), and find the linear correlation
        - have a kernel in the end to check the final result, if "failed" is returned, display a message to increase # of iterations


https://www.youtube.com/watch?v=I-W9jw7fwOg&list=PLqMASTX2e0SZb_ca77KBAHy10pz5vWEMp&index=5&t=0s

TODO: read these:
    https://devblogs.nvidia.com/how-overlap-data-transfers-cuda-cc/
    https://devblogs.nvidia.com/faster-parallel-reductions-kepler/
    https://devblogs.nvidia.com/finite-difference-methods-cuda-cc-part-1/   (from prof)
    https://devblogs.nvidia.com/efficient-matrix-transpose-cuda-cc/




# Issues:



TODO:       norm() add tree-like when adding the first threads of each block

TODO:       idea of optimization:
                - while foo is copied back and forth to host, do an async memcpy and continue with calculation
                    - doesn't matter if it's wasteful, can just quit if it has converged

            

CHECK:      When calculating the max_row_size on the device, we have to copy result back to CPU so that CPU will cudamalloc and cpy the required size for the vectors value and index
                - maybe not worthwhile to have it on the GPU?


TODO:       boundary condition (identity row)



TODO:       repair some reduction functions, add this in the beginning:

            __global__ 
            void sumOfVector_GPU(double* sum, double* x, size_t n)
            {
                int id = blockDim.x * blockIdx.x + threadIdx.x;
                int stride = blockDim.x*gridDim.x;
                
                __shared__ double cache[1024];
                cache[threadIdx.x] = 0;    < --- NOTE:
                
                double temp = 0.0;
                while(id < n)
                {
                    temp += x[id];


TODO:       repair PSFEM's calculateDimensions

            __host__ 
            void calculateDimensions(size_t N, dim3 &blockDim, dim3 &gridDim)   // TODO: have it gridDim first --> N, gridDim, blockDim
            {
                if ( N <= 1024 )
                {
                    blockDim.x = 1024; blockDim.y = 1; blockDim.z = 1; //NOTE: here, change N -> 1024, so that reductions work
                    gridDim.x  = 1; gridDim.y = 1; gridDim.z = 1;
                }

            .. because reduction kernels need full blocks to work



TODO:       use full u to calculate local driving forces
                - have a kernel to get only the local displacements

        

Questions:

compiler saves 0 sometimes in the 1e-9 region.
    - instead of say, 
        a == 0.0, 

        is this ok?
        
        a < 1e-8 && a > -1e-8

    in program, 0 becomes 7e-9

when initializing grid dimensions, Nx, Ny, Nz,
    - this is the base grid?



convergence is bad

compared to matlab, does this make sense?
 0 : res = 10000.000000
 1 : res = 3145.270263
 2 : res = 1237.000420
 3 : res = 521.989391
 4 : res = 227.646893
 5 : res = 100.942866
 6 : res = 45.208322
 7 : res = 20.387368
 8 : res = 9.241632
 9 : res = 4.206005
 10 : res = 1.920187
 11 : res = 0.878768
 12 : res = 0.402930
 13 : res = 0.185024
 14 : res = 0.085060
 15 : res = 0.039139
 16 : res = 0.018022
 17 : res = 0.008302
 18 : res = 0.003826
 19 : res = 0.001764
 20 : res = 0.000814
 21 : res = 0.000375
 22 : res = 0.000173
 23 : res = 0.000080
 24 : res = 0.000037
 25 : res = 0.000017
 26 : res = 0.000008
 27 : res = 0.000004
 28 : res = 0.000002
 29 : res = 0.000001
 30 : res = 0.000000
 31 : res = 0.000000

 

